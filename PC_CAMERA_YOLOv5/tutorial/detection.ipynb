{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@5.903] global /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_562_cazh1h/croots/recipe/opencv-suite_1664548333142/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "動体検知を開始します。\n",
      "2023-02-22 15:28:27.922667\n",
      "動体検知を終了します。\n",
      "2023-02-22 15:28:46.276122\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "# https://sasuwo.org/motion-detection/\n",
    "# python 物体検知 カメラ\n",
    "# 必要なライブラリのインポート\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "# Webカメラを使うときはこちら\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# ファイルパスで動画を読み込むときはこちら\n",
    "# filepath = \"読み込みたい動画ファイルのあるパスを入力する\"\n",
    "# cap = cv2.VideoCapture(filepath)\n",
    "\n",
    "\n",
    "before = None\n",
    "count = 1\n",
    "time = datetime.date.today()\n",
    "\n",
    "print(\"動体検知を開始します。\")\n",
    "print(str(datetime.datetime.now()))\n",
    "\n",
    "while True:\n",
    "    # 画像を取得\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # 再生が終了したらループを抜ける\n",
    "    if ret == False:\n",
    "        break\n",
    "\n",
    "    # 白黒画像に変換\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if before is None:\n",
    "        before = gray.astype(\"float\")\n",
    "        continue\n",
    "\n",
    "    # 現在のフレームと移動平均との差を計算\n",
    "    cv2.accumulateWeighted(gray, before, 0.5)\n",
    "    frameDelta = cv2.absdiff(gray, cv2.convertScaleAbs(before))\n",
    "\n",
    "    # frameDeltaの画像を２値化\n",
    "    thresh = cv2.threshold(frameDelta, 3, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    # 輪郭のデータを取得\n",
    "    contours = cv2.findContours(thresh,\n",
    "                    cv2.RETR_EXTERNAL,\n",
    "                    cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "    # 差分があった点を画面に描画\n",
    "    for target in contours:\n",
    "        x, y, w, h = cv2.boundingRect(target)\n",
    "        \n",
    "        # 小さい変更点は無視\n",
    "        if w < 30:\n",
    "            continue \n",
    "\n",
    "        areaframe = cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "\n",
    "        if count == 1 or count == 100:\n",
    "            cv2.imwrite('test.jpg',areaframe)\n",
    "        count = count + 1\n",
    "\n",
    "    # ウィンドウで表示\n",
    "    cv2.imshow('target_frame', frame)\n",
    "\n",
    "    # Enterキーが押されたらループを抜ける\n",
    "    if cv2.waitKey(1) == 13: break\n",
    "\n",
    "print(\"動体検知を終了します。\")\n",
    "print(str(datetime.datetime.now()))\n",
    "\n",
    "# ウィンドウの破棄\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yolov5 を実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunia/opt/anaconda3/envs/detection/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# model = torch.hub.load('../../yolov5/', 'yolov5s') \n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@613.569] global /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_562_cazh1h/croots/recipe/opencv-suite_1664548333142/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "動体検知を開始します。\n",
      "2023-02-21 17:01:00.928177\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_562_cazh1h/croots/recipe/opencv-suite_1664548333142/work/modules/imgproc/src/contours.cpp:197: error: (-210:Unsupported format or combination of formats) [Start]FindContours supports only CV_8UC1 images when mode != CV_RETR_FLOODFILL otherwise supports CV_32SC1 images only in function 'cvStartFindContours_Impl'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m thresh \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mthreshold(frameDelta, \u001b[39m3\u001b[39m, \u001b[39m255\u001b[39m, cv2\u001b[39m.\u001b[39mTHRESH_BINARY)[\u001b[39m1\u001b[39m]\n\u001b[1;32m     48\u001b[0m \u001b[39m# 輪郭のデータを取得\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m contours \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mfindContours(thresh,\n\u001b[1;32m     50\u001b[0m                 cv2\u001b[39m.\u001b[39;49mRETR_EXTERNAL,\n\u001b[1;32m     51\u001b[0m                 cv2\u001b[39m.\u001b[39;49mCHAIN_APPROX_SIMPLE)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     53\u001b[0m \u001b[39m# 差分があった点を画面に描画\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m target \u001b[39min\u001b[39;00m contours:\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_562_cazh1h/croots/recipe/opencv-suite_1664548333142/work/modules/imgproc/src/contours.cpp:197: error: (-210:Unsupported format or combination of formats) [Start]FindContours supports only CV_8UC1 images when mode != CV_RETR_FLOODFILL otherwise supports CV_32SC1 images only in function 'cvStartFindContours_Impl'\n"
     ]
    }
   ],
   "source": [
    "# https://sasuwo.org/motion-detection/\n",
    "# python 物体検知 カメラ\n",
    "# 必要なライブラリのインポート\n",
    "\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "# Webカメラを使うときはこちら\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# ファイルパスで動画を読み込むときはこちら\n",
    "# filepath = \"読み込みたい動画ファイルのあるパスを入力する\"\n",
    "# cap = cv2.VideoCapture(filepath)\n",
    "\n",
    "\n",
    "before = None\n",
    "count = 1\n",
    "time = datetime.date.today()\n",
    "\n",
    "print(\"動体検知を開始します。\")\n",
    "print(str(datetime.datetime.now()))\n",
    "\n",
    "while True:\n",
    "    # 画像を取得\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # 再生が終了したらループを抜ける\n",
    "    if ret == False:\n",
    "        break\n",
    "\n",
    "    # 白黒画像に変換\n",
    "    # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "    if before is None:\n",
    "        before = img.astype(\"float\")\n",
    "        continue\n",
    "\n",
    "    # 現在のフレームと移動平均との差を計算\n",
    "    # cv2.accumulateWeighted(gray, before, 0.5)\n",
    "    # frameDelta = cv2.absdiff(gray, cv2.convertScaleAbs(before))\n",
    "    cv2.accumulateWeighted(img, before, 0.5)\n",
    "    frameDelta = cv2.absdiff(img, cv2.convertScaleAbs(before))\n",
    "\n",
    "    # frameDeltaの画像を２値化\n",
    "    thresh = cv2.threshold(frameDelta, 3, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    # 輪郭のデータを取得\n",
    "    contours = cv2.findContours(thresh,\n",
    "                    cv2.RETR_EXTERNAL,\n",
    "                    cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "    # 差分があった点を画面に描画\n",
    "    for target in contours:\n",
    "        x, y, w, h = cv2.boundingRect(target)\n",
    "        \n",
    "        # 小さい変更点は無視\n",
    "        if w < 30:\n",
    "            continue \n",
    "\n",
    "        areaframe = cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "\n",
    "        if count == 1 or count == 100:\n",
    "            cv2.imwrite('test.jpg',areaframe)\n",
    "        count = count + 1\n",
    "\n",
    "    # ウィンドウで表示\n",
    "    cv2.imshow('target_frame', frame)\n",
    "\n",
    "    # Enterキーが押されたらループを抜ける\n",
    "    if cv2.waitKey(1) == 13: break\n",
    "\n",
    "print(\"動体検知を終了します。\")\n",
    "print(str(datetime.datetime.now()))\n",
    "\n",
    "# ウィンドウの破棄\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:24:02) \n[Clang 11.1.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7b63737d34f860a632a8143e822850a892e75196c8e7207c138f17d21e5a1d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### チュートリアルの学習\n",
    "- https://yutaroogawa.github.io/pytorch_tutorials_jp/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 442.7127760849192\n",
      "199 2.4212810292177798\n",
      "299 0.026646992524446286\n",
      "399 0.0003348938253444464\n",
      "499 4.357874504552627e-06\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N：バッチサイズ         D_in：入力層の次元数\n",
    "# H：隠れ層の次元数       D_out： 出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 乱数により入力データと目標となる出力データを生成\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 乱数による重みの初期化\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 順伝播： 予測値yの計算\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # 損失の計算と表示\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 逆伝搬：損失に対するW1とw2の勾配の計算\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 重みの更新\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習前出力：[-164.37    9.64   34.79  510.17    4.69 -316.84   12.82  201.45 -273.12\n",
      "  555.25]\n",
      "学習後出力：[-0.73  0.83  0.05  0.35 -0.35  0.48 -0.15  0.47  1.06  0.65]\n",
      "目的の出力：[-0.73  0.83  0.05  0.35 -0.35  0.48 -0.15  0.47  1.06  0.65]\n"
     ]
    }
   ],
   "source": [
    "#日本語訳追記：\n",
    "#学習前の重みを定義（前の学習で使用された重みの初期値とは異なります）\n",
    "w1_unlearned = np.random.randn(D_in, H)\n",
    "w2_unlearned = np.random.randn(H, D_out)\n",
    "\n",
    "#学習前の出力を算出\n",
    "h = x.dot(w1_unlearned)\n",
    "h_relu = np.maximum(h, 0)\n",
    "y_pred = h_relu.dot(w2_unlearned)\n",
    "print(f\"学習前出力：{np.round(y_pred[0], decimals=2)}\")\n",
    "\n",
    "#学習によって得られた重みで出力を算出\n",
    "h = x.dot(w1)\n",
    "h_relu = np.maximum(h, 0)\n",
    "y_pred = h_relu.dot(w2)\n",
    "print(f\"学習後出力：{np.round(y_pred[0], decimals=2)}\")\n",
    "\n",
    "#目的の出力yとの比較\n",
    "print(f\"目的の出力：{np.round(y[0], decimals=2)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 710.273193359375\n",
      "199 4.414570331573486\n",
      "299 0.03892833739519119\n",
      "399 0.000632779614534229\n",
      "499 7.131210441002622e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# N：バッチサイズ         D_in：入力層の次元数\n",
    "# H：隠れ層の次元数       D_out： 出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 乱数により入力データと目標となる出力データを生成\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 乱数による重みの初期化\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 順伝播： 予測値yの計算\n",
    "    # mm() は行列の掛け算\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    # x * w1 * w2 -> 予測値 y^\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 損失の計算と表示\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "   # 逆伝搬：損失に対するW1とw2の勾配の計算\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 確率的勾配降下法による重みの更新\n",
    "    # grad は torch 同志をかけた時に生じる（？）\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 862.5645751953125\n",
      "tensor([[ 3.6285e+01, -8.8487e+00,  1.5507e+01,  ..., -2.6105e+01,\n",
      "         -1.5346e+00, -1.3366e+01],\n",
      "        [ 5.6110e+01, -3.1637e+01, -1.3721e+01,  ..., -4.7835e+01,\n",
      "          3.5215e+00, -8.2975e-01],\n",
      "        [-1.7398e+01,  4.5638e+00, -1.1748e+01,  ..., -1.8335e+01,\n",
      "         -7.6204e-01,  1.0017e+01],\n",
      "        ...,\n",
      "        [ 8.3582e+01, -3.8635e+01, -3.1670e+00,  ..., -3.0016e+01,\n",
      "          5.5061e+00, -3.3824e+01],\n",
      "        [ 3.1997e+01, -2.3670e+00,  5.4479e+00,  ..., -6.3684e+00,\n",
      "         -1.5295e+00,  1.2324e+01],\n",
      "        [ 1.8240e-02,  2.8511e+01,  2.8008e+00,  ..., -3.8512e+01,\n",
      "          5.0114e+00, -1.9621e+01]], device='mps:0')\n",
      "199 12.628241539001465\n",
      "tensor([[ 1.6824e+00, -9.6306e-01,  1.4818e+00,  ...,  2.3631e+00,\n",
      "          2.5596e-01, -1.1032e+00],\n",
      "        [ 7.3589e+00, -3.2366e+00, -1.4048e-01,  ..., -7.9956e+00,\n",
      "         -2.5128e-01, -3.0169e+00],\n",
      "        [ 1.4752e+00,  2.4065e-01, -8.5296e-03,  ..., -6.0769e+00,\n",
      "         -2.5394e-01, -1.5510e+00],\n",
      "        ...,\n",
      "        [ 8.4793e+00, -4.5409e+00,  4.9653e-02,  ..., -2.9709e+00,\n",
      "         -1.4650e-01, -2.5015e+00],\n",
      "        [ 9.0630e+00, -1.9296e+00,  1.1431e+00,  ..., -9.9439e-01,\n",
      "          6.2112e-01, -1.0665e+00],\n",
      "        [-6.8142e-01,  2.9774e+00,  3.4109e-01,  ..., -3.5517e+00,\n",
      "         -1.1683e-01, -2.5596e+00]], device='mps:0')\n",
      "299 0.40654951333999634\n",
      "tensor([[-0.0730, -0.0129,  0.1967,  ...,  0.9795,  0.0154, -0.0178],\n",
      "        [ 1.0977, -0.3833,  0.0018,  ..., -1.2942, -0.0502, -0.7714],\n",
      "        [ 0.5400,  0.0111,  0.0662,  ..., -1.2968, -0.0416, -0.5774],\n",
      "        ...,\n",
      "        [ 1.2356, -0.6410,  0.0041,  ..., -0.3749, -0.0338, -0.2893],\n",
      "        [ 1.3711, -0.3095,  0.1978,  ..., -0.0593,  0.1137, -0.3128],\n",
      "        [ 0.0364,  0.4088,  0.0741,  ..., -0.4501, -0.0796, -0.4920]],\n",
      "       device='mps:0')\n",
      "399 0.016846423968672752\n",
      "tensor([[-5.0991e-02,  1.7315e-02,  3.3017e-02,  ...,  2.3873e-01,\n",
      "          1.8382e-03,  1.2420e-02],\n",
      "        [ 1.8413e-01, -5.5605e-02,  1.7097e-03,  ..., -2.3032e-01,\n",
      "         -1.0823e-02, -1.6990e-01],\n",
      "        [ 1.3616e-01, -2.0762e-04,  2.0585e-02,  ..., -2.6966e-01,\n",
      "         -7.5832e-03, -1.4050e-01],\n",
      "        ...,\n",
      "        [ 2.2526e-01, -1.1080e-01,  7.8910e-04,  ..., -5.9921e-02,\n",
      "         -4.4795e-03, -4.4550e-02],\n",
      "        [ 2.3456e-01, -4.9581e-02,  4.0502e-02,  ...,  5.7312e-03,\n",
      "          2.4276e-02, -6.5554e-02],\n",
      "        [ 2.6614e-02,  6.6483e-02,  2.2760e-02,  ..., -6.6245e-02,\n",
      "         -2.2711e-02, -1.0640e-01]], device='mps:0')\n",
      "499 0.0010420135222375393\n",
      "tensor([[-0.0190,  0.0061,  0.0094,  ...,  0.0654,  0.0021,  0.0074],\n",
      "        [ 0.0377, -0.0120, -0.0010,  ..., -0.0546, -0.0014, -0.0398],\n",
      "        [ 0.0387, -0.0041,  0.0082,  ..., -0.0668, -0.0019, -0.0362],\n",
      "        ...,\n",
      "        [ 0.0492, -0.0263, -0.0024,  ..., -0.0131, -0.0015, -0.0085],\n",
      "        [ 0.0525, -0.0126,  0.0092,  ...,  0.0060,  0.0070, -0.0166],\n",
      "        [ 0.0051,  0.0143,  0.0078,  ..., -0.0170, -0.0066, -0.0275]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# N：バッチサイズ         D_in：入力層の次元数\n",
    "# H：隠れ層の次元数       D_out： 出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 日本語訳注：デフォルトでは requires_grad は False が設定されています。\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 乱数による重みを表すTensorの定義\n",
    "# 逆伝播の際、このTensorに対する勾配を計算する場合は、requires_grad=Trueを指定します\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 順伝播： Tensorの演算を利用して予測結果yの算出\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Tensorを用いた損失の計算と表示\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograd を使用して逆伝播の計算をします。\n",
    "    # backward()によりrequires_gradにTrueが設定されているすべてのTensorに対して、\n",
    "    # 損失の勾配を計算を行います。これにより、w1.gradとw2.gradはそれぞれw1とw2に\n",
    "    # 対する損失の勾配を保持するTensorになります。\n",
    "    loss.backward()\n",
    "\n",
    "    # 確率的勾配降下法を使って手動で重みを更新します。\n",
    "    # 別の方法としては、weight.dataとweight.grad.dataを利用する方法があります。\n",
    "    # tensor.dataはtensorと保存領域を共有するTensorを返しますが、履歴は追跡されません。\n",
    "    # torch.optim.SGDを利用して追跡を回避することも可能です。\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 重みの更新後、手動で勾配を0に初期化\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: 新しいautograd関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/np/chqdmqld4h98fpysyglct_4m0000gp/T/ipykernel_63103/1518199411.py:27: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525849783/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  grad_input[input < 0] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 748.716552734375\n",
      "199 6.433542251586914\n",
      "299 0.11321971565485\n",
      "399 0.002757652197033167\n",
      "499 0.00021390017354860902\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    torch.autograd.Functionをサブクラス化し、Tensors上で動作する順伝播経路と\n",
    "    逆伝播経路を定義することで、独自のautograd Functionsを実装することができます。\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        順伝播の経路では入力を含むTensorを受け取り、出力を含むTensorを返します。 \n",
    "        ctxは逆伝播の際に必要な情報を格納するコンテキストオブジェクトです。\n",
    "        ctx.save_for_backwardメソッドを使用して、任意のオブジェクトを一時的に保持し、\n",
    "        逆伝播時に使用することができます。\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        逆伝播の経路では、出力に対する損失の勾配を含むTensorを受け取り、\n",
    "        入力に対する損失の勾配を計算する必要があります。\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"mps\")\n",
    "# 上の行でTensorFloat32を無効にします。\n",
    "# TensorFloat32を用いると精度を犠牲にして、ネットワークの高速化を図ることができます。\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "\n",
    "# N：バッチサイズ         D_in：入力層の次元数\n",
    "# H：隠れ層の次元数       D_out： 出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 乱数により入力データと目標となる出力データを表すTensorを生成\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 乱数による重みを表すTensorの定義\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 関数を適用するには、Function.applyメソッドを用います。\n",
    "    relu = MyReLU.apply\n",
    "    # <built-in method apply of FunctionMeta object at 0x1420fc4f0>\n",
    "\n",
    "    # 順伝播：独自のautograd操作を用いてReLUの出力を算出することで予想結果yを計算します。\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # 損失の計算と表示\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # 確率的勾配降下法を用いた重みの更新\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 重みの更新後、手動で勾配を0に初期化\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nnモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.690983772277832\n",
      "199 0.039553556591272354\n",
      "299 0.0013074190355837345\n",
      "399 6.767728336853907e-05\n",
      "499 4.4400617298379075e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N：バッチサイズ         D_in：入力層の次元数\n",
    "# H：隠れ層の次元数       D_out： 出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nnパッケージを利用し、レイヤーの連なりとしてモデルを定義します。\n",
    "# nn.Sequentialは他のモジュールを並べて保持することで、それぞれのモジュールを順番に\n",
    "# 実行し、出力を得ます。各Linearモジュールは線形関数を使用して入力から出力を計算し、\n",
    "# 重みとバイアスを内部のTensorで保持します。\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 損失の計算と表示\n",
    "    # 損失関数にyの予測値と正解の値を持つTensorを渡すことで損失を持つTenosrを\n",
    "    # 得ることができます。\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # 確率的勾配降下法を用いた重みの更新\n",
    "    # 各々のパラメータはTensorなので、これまでと同じ方法で勾配を参照することができます。\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 51.07844543457031\n",
      "199 0.8447763919830322\n",
      "299 0.0031095314770936966\n",
      "399 8.619559594080783e-06\n",
      "499 3.07811198752006e-08\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N：バッチサイズ         D_in：入力層の次元数\n",
    "# H：隠れ層の次元数       D_out： 出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# optimパッケージを使用して、モデルの重みを更新するオプティマイザを定義します。\n",
    "# ここではAdamを使用します。optimパッケージには他にも多くの最適化アルゴリズムが存在ます。\n",
    "# Adamのコンストラクタの最初の引数により、オプティマイザがどのTensorを更新するか指定できます。\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 損失の計算と表示\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 逆伝播に入る前に、更新されることになる変数（モデルの学習可能な重み）の勾配を\n",
    "    # optimaizerを使用して0に初期化します。\n",
    "    # これは、デフォルトで.backward()が呼び出される度に勾配がバッファに蓄積されるため\n",
    "    # 必要になる操作です（上書きされるわけではない）。\n",
    "    # 詳しくはtorch.autograd.backwardのドキュメントを参照してください。\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 逆伝播：モデルのパラメータに対応する損失の勾配を計算\n",
    "    loss.backward()\n",
    "\n",
    "    # オプティマイザのstep関数を呼び出すことでパラメータを更新\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0320,  0.0045,  0.0301,  ..., -0.0010, -0.0029,  0.0022],\n",
       "         [-0.0030, -0.0179, -0.0186,  ..., -0.0099, -0.0111,  0.0371],\n",
       "         [ 0.0050, -0.0239,  0.0169,  ..., -0.0255, -0.0134, -0.0177],\n",
       "         ...,\n",
       "         [ 0.0035, -0.0040, -0.0224,  ...,  0.0293, -0.0314,  0.0327],\n",
       "         [-0.0207,  0.0050, -0.0247,  ..., -0.0122,  0.0124, -0.0165],\n",
       "         [ 0.0225, -0.0106,  0.0084,  ..., -0.0049, -0.0001,  0.0035]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0414, -0.0166,  0.0062, -0.0093,  0.0158,  0.0222,  0.0095,  0.0297,\n",
       "          0.0428,  0.0347,  0.0297,  0.0275,  0.0386,  0.0147,  0.0252,  0.0105,\n",
       "          0.0229,  0.0118,  0.0449, -0.0222,  0.0161,  0.0393,  0.0156, -0.0067,\n",
       "         -0.0071,  0.0148,  0.0239,  0.0252,  0.0370, -0.0150,  0.0085,  0.0097,\n",
       "          0.0164, -0.0106,  0.0214, -0.0147, -0.0131,  0.0112,  0.0109,  0.0130,\n",
       "          0.0431, -0.0171,  0.0323, -0.0020,  0.0028, -0.0156,  0.0203,  0.0236,\n",
       "         -0.0042,  0.0152, -0.0222,  0.0159,  0.0130,  0.0149, -0.0114,  0.0339,\n",
       "          0.0050,  0.0117, -0.0165,  0.0052,  0.0150, -0.0255, -0.0031,  0.0218,\n",
       "          0.0076, -0.0175,  0.0401, -0.0195,  0.0410,  0.0062,  0.0195,  0.0102,\n",
       "         -0.0185, -0.0163,  0.0180,  0.0215,  0.0020,  0.0020,  0.0316,  0.0294,\n",
       "          0.0347,  0.0347, -0.0025, -0.0070,  0.0303, -0.0087,  0.0253,  0.0191,\n",
       "          0.0185,  0.0003,  0.0266, -0.0263,  0.0023, -0.0143,  0.0008,  0.0336,\n",
       "          0.0002,  0.0331,  0.0143,  0.0242], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0350, -0.0349,  0.0282, -0.0326, -0.0261, -0.0314, -0.0905, -0.0704,\n",
       "           0.0103,  0.0813,  0.0335,  0.0236,  0.0198, -0.0852,  0.0198,  0.0373,\n",
       "           0.0477,  0.0040,  0.0252, -0.0610, -0.0880, -0.0995, -0.0932,  0.0611,\n",
       "          -0.0979,  0.0514,  0.1095,  0.0760,  0.0405,  0.0309, -0.0720, -0.0941,\n",
       "          -0.0308,  0.0173, -0.0352,  0.0030,  0.0426,  0.0695, -0.0757, -0.0928,\n",
       "          -0.1049,  0.1052,  0.0705,  0.0107, -0.0819,  0.0828,  0.0766, -0.0277,\n",
       "           0.0348, -0.0479, -0.0741,  0.0240,  0.0523,  0.0918,  0.1071,  0.0201,\n",
       "           0.0778,  0.0402,  0.0930, -0.0634,  0.0947,  0.0631,  0.0956,  0.0506,\n",
       "          -0.0541, -0.0138, -0.0481, -0.0318, -0.0559,  0.0544, -0.0887,  0.0448,\n",
       "           0.0324, -0.0325, -0.0808, -0.0370,  0.0861,  0.0081,  0.0054, -0.1097,\n",
       "          -0.0616, -0.0157, -0.0420,  0.0813,  0.0220,  0.0884,  0.0454, -0.0813,\n",
       "          -0.1070, -0.0171,  0.0045, -0.0211, -0.0833,  0.0563,  0.0377,  0.0277,\n",
       "          -0.0819,  0.0740,  0.0759, -0.0626],\n",
       "         [ 0.0726, -0.1203,  0.0228, -0.0566, -0.0193, -0.0925, -0.0355, -0.0429,\n",
       "           0.0101,  0.0373,  0.0291,  0.0136,  0.0678,  0.0360, -0.0728, -0.1129,\n",
       "           0.0275,  0.0753, -0.0357,  0.0899, -0.0511, -0.1049,  0.0493, -0.0574,\n",
       "          -0.0998, -0.0790, -0.0069,  0.0557,  0.0597, -0.0814, -0.0665, -0.0473,\n",
       "           0.0603, -0.0013,  0.0540,  0.0982,  0.0460,  0.0336, -0.0453, -0.0608,\n",
       "          -0.0299, -0.0892, -0.0021,  0.0355,  0.0663,  0.0334,  0.0756, -0.0925,\n",
       "           0.0912, -0.0802,  0.0095,  0.0937, -0.0295, -0.0982, -0.0970,  0.0405,\n",
       "          -0.0988, -0.0376,  0.0583,  0.0289,  0.0711, -0.0227,  0.0150, -0.0469,\n",
       "          -0.0435, -0.0088, -0.0585, -0.0486, -0.0205,  0.0469, -0.0434,  0.0903,\n",
       "           0.0110,  0.0017,  0.0744, -0.0269,  0.0040, -0.0060, -0.0691,  0.0925,\n",
       "           0.0359, -0.0897,  0.0898,  0.0671, -0.0176,  0.0690, -0.0347,  0.0841,\n",
       "          -0.0303,  0.1007,  0.1022, -0.0869,  0.0542,  0.0519,  0.0538,  0.0208,\n",
       "           0.0549, -0.0893,  0.0988,  0.0257],\n",
       "         [ 0.0039,  0.0231,  0.0404, -0.0492,  0.0988, -0.0124,  0.0518, -0.1061,\n",
       "          -0.0775, -0.0667, -0.0759, -0.0034,  0.0930, -0.0874, -0.0965, -0.0450,\n",
       "          -0.0822,  0.0584, -0.1004,  0.0981,  0.0617, -0.1025, -0.1059, -0.0408,\n",
       "          -0.0411,  0.0493, -0.0621,  0.0622,  0.0129, -0.1095,  0.0120, -0.0278,\n",
       "           0.0294,  0.0821,  0.0309,  0.0238,  0.0207, -0.1134,  0.0886,  0.0758,\n",
       "           0.0512,  0.0006, -0.0276, -0.0527, -0.0426,  0.0361, -0.0433, -0.0290,\n",
       "           0.0197, -0.0487, -0.0043, -0.0165,  0.0146, -0.0078, -0.0334, -0.0824,\n",
       "           0.0908, -0.0982, -0.1146, -0.0868, -0.0509,  0.0576, -0.0426,  0.0673,\n",
       "           0.0117,  0.0891,  0.0839,  0.0263, -0.1081,  0.0394, -0.0893, -0.0648,\n",
       "           0.0660, -0.0945,  0.0628,  0.1084,  0.0624,  0.1103,  0.0557, -0.0978,\n",
       "           0.0971, -0.0157, -0.0666,  0.0191,  0.0645,  0.0403,  0.0745,  0.0519,\n",
       "          -0.1078,  0.0803,  0.0260,  0.1017,  0.0017,  0.0191, -0.0475, -0.0633,\n",
       "          -0.0312,  0.0809,  0.0578,  0.0922],\n",
       "         [-0.0475, -0.1021,  0.0525, -0.0490, -0.1119,  0.0050, -0.0096, -0.0763,\n",
       "           0.0634,  0.1107,  0.0411, -0.0442, -0.0431,  0.0711,  0.0977,  0.0191,\n",
       "           0.0253,  0.0983, -0.0773,  0.0676,  0.0338,  0.0589, -0.0019, -0.0878,\n",
       "          -0.0720, -0.0114, -0.0658, -0.0982,  0.1114,  0.1095,  0.0370, -0.0446,\n",
       "           0.0553,  0.1087, -0.0880,  0.0948, -0.0701, -0.0203,  0.0887, -0.0739,\n",
       "          -0.0639, -0.0618,  0.0491, -0.0646,  0.0925, -0.1042, -0.1079,  0.0098,\n",
       "           0.0942, -0.0293,  0.0823,  0.0011,  0.0245,  0.0564, -0.0329,  0.0840,\n",
       "           0.0412, -0.0480,  0.0392,  0.0707, -0.0689,  0.0614, -0.0312,  0.0856,\n",
       "          -0.1087, -0.0859, -0.0572, -0.0322,  0.0871,  0.0581,  0.0724, -0.0246,\n",
       "          -0.0005,  0.0595,  0.0245,  0.0289,  0.0785, -0.0941,  0.0960,  0.0655,\n",
       "           0.0840, -0.0684, -0.0044,  0.0743, -0.0921, -0.0576, -0.0314,  0.0261,\n",
       "           0.0681,  0.0406, -0.1086,  0.0134,  0.0904, -0.1092, -0.0073,  0.0170,\n",
       "          -0.0758, -0.0366, -0.0229, -0.0232],\n",
       "         [-0.0889, -0.0414, -0.0789, -0.0845,  0.0518,  0.0069, -0.1004,  0.0183,\n",
       "          -0.1035, -0.0116,  0.0393,  0.0990,  0.0610,  0.0761,  0.0518, -0.0116,\n",
       "          -0.0996,  0.0167, -0.1001, -0.0782,  0.0391,  0.0625,  0.0515,  0.0642,\n",
       "          -0.0914,  0.1061, -0.0755, -0.1064, -0.0638, -0.0987,  0.0790, -0.0840,\n",
       "           0.0048,  0.0269,  0.0946,  0.0233,  0.0774, -0.0199, -0.0857, -0.0825,\n",
       "           0.0510, -0.0385, -0.0263, -0.0880,  0.1002, -0.0830,  0.0883, -0.0483,\n",
       "           0.0906,  0.0328,  0.0914, -0.0110,  0.0585,  0.1062, -0.0972,  0.0850,\n",
       "          -0.0088,  0.0501,  0.0639, -0.0533,  0.1162, -0.0879, -0.0925,  0.0595,\n",
       "           0.1154, -0.1063, -0.0972, -0.1095,  0.0739, -0.0423,  0.0172,  0.0177,\n",
       "          -0.0746,  0.0330, -0.0328,  0.0808, -0.0610,  0.0295,  0.0528,  0.1083,\n",
       "          -0.0297,  0.0489, -0.1084, -0.0795,  0.0106,  0.0536, -0.0410,  0.0945,\n",
       "           0.0886, -0.0172,  0.0269,  0.0396, -0.0669,  0.0822,  0.0196,  0.0909,\n",
       "          -0.0255, -0.0153,  0.0166,  0.0701],\n",
       "         [ 0.0702, -0.0873,  0.0131,  0.0232,  0.0954,  0.0025,  0.0896, -0.0821,\n",
       "           0.0805, -0.0908, -0.1002, -0.0654,  0.0283, -0.0199, -0.0253, -0.0551,\n",
       "          -0.0554,  0.0843,  0.1033, -0.0511,  0.0530,  0.0246,  0.0177,  0.0477,\n",
       "          -0.0400, -0.1097,  0.0439, -0.0562, -0.1075,  0.0559, -0.0689, -0.1134,\n",
       "           0.1066, -0.0641,  0.0791, -0.0380, -0.0331, -0.0335, -0.0942,  0.0944,\n",
       "           0.0621, -0.1128,  0.0755, -0.0645, -0.0213, -0.0558,  0.0084,  0.0440,\n",
       "           0.0875,  0.0174,  0.1065,  0.0104, -0.0433,  0.0436, -0.0947, -0.0521,\n",
       "          -0.0013,  0.0031,  0.0629,  0.0912, -0.0100, -0.0060,  0.0014, -0.0992,\n",
       "           0.0009,  0.0738, -0.0689, -0.0217,  0.0378, -0.1105,  0.0328, -0.0226,\n",
       "           0.0964, -0.0723, -0.1070,  0.0845,  0.1057, -0.0498,  0.0602, -0.0348,\n",
       "          -0.0615,  0.0446,  0.0045,  0.1033, -0.1044,  0.0825,  0.0950, -0.0811,\n",
       "           0.0665, -0.0908,  0.0106,  0.0525, -0.0653,  0.0664,  0.1137, -0.0863,\n",
       "          -0.1038, -0.0363, -0.0679,  0.0657],\n",
       "         [-0.0872,  0.0445,  0.0035,  0.0357, -0.0932,  0.0372, -0.1010, -0.0768,\n",
       "          -0.0684, -0.0254,  0.0734,  0.0393, -0.0524,  0.0594,  0.1034, -0.0443,\n",
       "           0.1039, -0.0327, -0.0200,  0.0362, -0.1102, -0.0403,  0.0418, -0.0248,\n",
       "           0.1090, -0.0343, -0.0146,  0.0031,  0.1050,  0.0502, -0.1021,  0.0876,\n",
       "           0.0912,  0.0308, -0.0729, -0.0821, -0.0142, -0.0209, -0.0447,  0.0105,\n",
       "           0.0226, -0.0494, -0.1170, -0.0876, -0.0287,  0.0771,  0.0911,  0.0841,\n",
       "          -0.0515, -0.0769, -0.0263,  0.0948,  0.0688,  0.0018, -0.0526, -0.0046,\n",
       "          -0.0329, -0.0263,  0.0840,  0.0168, -0.0074, -0.0354, -0.0343, -0.0927,\n",
       "          -0.0367,  0.0995, -0.0902,  0.0682,  0.0226, -0.0950,  0.1097,  0.0607,\n",
       "          -0.1078, -0.0159,  0.0012,  0.0341,  0.0513, -0.0687, -0.1084, -0.0070,\n",
       "           0.1039,  0.0815, -0.0858, -0.0315, -0.0243,  0.0117,  0.0539, -0.0471,\n",
       "           0.0550, -0.0290, -0.0394, -0.0789, -0.0519,  0.0794,  0.0115, -0.0469,\n",
       "           0.0802, -0.0687,  0.1151,  0.0486],\n",
       "         [-0.1137,  0.1090, -0.1133, -0.0648,  0.0827,  0.0683, -0.0155,  0.0883,\n",
       "          -0.0793,  0.0379, -0.0957,  0.0408,  0.1137, -0.0854, -0.0149, -0.0439,\n",
       "          -0.0596, -0.0495,  0.0082, -0.0127,  0.0318, -0.0581,  0.0176, -0.0223,\n",
       "           0.1072,  0.0877,  0.0186, -0.0739, -0.0023, -0.0262, -0.0741, -0.0049,\n",
       "          -0.0570,  0.0641,  0.0520,  0.0911, -0.0808, -0.0016, -0.0968,  0.0178,\n",
       "          -0.1060,  0.0383,  0.0825,  0.0721,  0.0067, -0.0633, -0.0292, -0.0568,\n",
       "           0.0527, -0.0622, -0.0221, -0.0600,  0.1062,  0.0563,  0.0902,  0.1031,\n",
       "           0.0571, -0.0568,  0.0040,  0.0904, -0.0745, -0.0065,  0.0527, -0.0405,\n",
       "          -0.0442, -0.0320, -0.0163, -0.0421, -0.1039,  0.0560, -0.0460, -0.0246,\n",
       "           0.0843,  0.0369,  0.0200, -0.0166, -0.0649, -0.1025, -0.0811,  0.1097,\n",
       "          -0.0432,  0.0335,  0.0494, -0.0594,  0.0729, -0.0558, -0.0106,  0.0379,\n",
       "          -0.1042,  0.0538, -0.0041,  0.0337,  0.0862,  0.0135,  0.1098,  0.0425,\n",
       "           0.1006, -0.0732, -0.0889,  0.1060],\n",
       "         [ 0.0510, -0.0572, -0.0334,  0.0769, -0.0199,  0.0429, -0.0837,  0.0797,\n",
       "           0.0049, -0.0074,  0.1106, -0.0150, -0.0412,  0.0887, -0.0094, -0.0351,\n",
       "           0.0942,  0.1069,  0.0914,  0.0070, -0.0460, -0.0791, -0.0755,  0.0599,\n",
       "           0.0295,  0.0533, -0.0594, -0.0626, -0.0060,  0.0116, -0.0986, -0.0479,\n",
       "          -0.0102,  0.0202, -0.0299, -0.0520, -0.0776,  0.0540, -0.1125, -0.0391,\n",
       "           0.1013,  0.0202,  0.0157, -0.0137,  0.0104, -0.0068,  0.0813,  0.0795,\n",
       "           0.0514,  0.0706, -0.0616,  0.0837, -0.0114, -0.0186, -0.0875,  0.1156,\n",
       "           0.0930,  0.0052, -0.0273, -0.0039, -0.1121, -0.0827,  0.1124,  0.0642,\n",
       "          -0.0898, -0.0853,  0.0336, -0.0004,  0.0101,  0.0918, -0.1013, -0.0447,\n",
       "          -0.0076, -0.0870, -0.0951, -0.0023,  0.0399, -0.0451,  0.0627, -0.0682,\n",
       "           0.0883, -0.0183,  0.0950,  0.0199,  0.0801, -0.0615, -0.0595,  0.0772,\n",
       "          -0.1053,  0.0749, -0.0699, -0.0439, -0.0467,  0.0201, -0.0292,  0.0868,\n",
       "          -0.0663, -0.0539,  0.0787,  0.0180],\n",
       "         [-0.0474,  0.0288,  0.0968, -0.0287,  0.0355, -0.0699,  0.0711,  0.0078,\n",
       "           0.1109,  0.0407, -0.0524, -0.0551,  0.0114,  0.0586,  0.0534, -0.0251,\n",
       "          -0.0850,  0.0547,  0.0733, -0.1128,  0.0518,  0.0430, -0.1079, -0.0816,\n",
       "          -0.0444, -0.0495, -0.0325, -0.0397, -0.0085, -0.1016, -0.1099, -0.0626,\n",
       "           0.0520,  0.0986, -0.0807, -0.0069, -0.0885,  0.0557,  0.0897,  0.0508,\n",
       "          -0.0955, -0.0038,  0.0126,  0.0043,  0.0560,  0.0229, -0.0297, -0.0449,\n",
       "          -0.0255,  0.0845, -0.1043, -0.0722,  0.0829,  0.0741,  0.0144,  0.0265,\n",
       "           0.0765, -0.0549, -0.0568,  0.0036,  0.0220,  0.0550,  0.0934,  0.0724,\n",
       "           0.0292, -0.0504,  0.0262,  0.0222,  0.0304,  0.0751,  0.0479,  0.0841,\n",
       "           0.0900,  0.0130, -0.0933, -0.1054,  0.0834,  0.0362,  0.0441,  0.0581,\n",
       "          -0.0276, -0.0750, -0.0593, -0.0777,  0.0814,  0.0863, -0.0919, -0.0700,\n",
       "           0.0418,  0.0582,  0.0703,  0.0471, -0.0387, -0.0918,  0.0405, -0.0340,\n",
       "          -0.0537, -0.0281, -0.0381,  0.0693]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0351,  0.0775, -0.0804, -0.0094,  0.0274,  0.0584,  0.0876, -0.0379,\n",
       "         -0.0026,  0.0548], requires_grad=True)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7b63737d34f860a632a8143e822850a892e75196c8e7207c138f17d21e5a1d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
